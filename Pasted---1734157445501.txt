# 処理概要

## 全体の目的

- 膨大な法令文書群および社内規定文書群に対し、**抽象的要求事項を満たしているか、禁止事項に反していないか**を自動的に確認する。
- テキスト分割や要求抽出、再評価、レポート生成などの過程で、`gpt-4o`を活用する。
- 文書が非常に大きく、`gpt-4o`のトークン上限に直接かけることが困難な場合は、**段階的かつ階層的な分割手法**を用いることで対応する。

## 主な特徴

1. **段階的なテキスト分割**:  
   巨大文書を機械的な物理分割（トークン数や文字数基準）→ `gpt-4o`による意味単位抽出 → 結果統合 → 再分割… という手順で階層的に論理構造を抽出する。
   
2. **要求抽出と具体化**:  
   `gpt-4o`を用いて、法令に記載された要求事項や禁止事項を抽出、抽象的要求を具体策へブレークダウン。

3. **Embeddingとクラスタリング**:  
   規定文書をチャンク分割し、Embeddingベクトル化およびクラスタリングにより大規模な文書を効率的に分析。

4. **LLMによる再評価、Reranking**:  
   類似度検索後、`gpt-4o`で要求事項と規定文書記載を照らし合わせ、最終的な適合性判断。

5. **レポート出力**:  
   `gpt-4o`で最終的なギャップ分析結果や改善提案をレポート化。

---

## ステップ詳細

### 1. 入力前処理・階層的テキスト分割

#### (1) 初期テキスト抽出・粗分割

- **処理内容**:  
  - 法令PDF/HTMLからテキスト化  
  - 規定PDF/HTMLからテキスト化  
  - 各テキストを約5000トークン程度になるように機械的チャンク分割 (tiktokenなどでトークン数見積もり)

- **出力**:  
  - `hourei_initial_chunks` (法令用粗チャンクリスト)  
  - `kitei_initial_chunks` (規定用粗チャンクリスト)

#### (2) `gpt-4o`を用いた上位セクション抽出(法令側)

- **処理内容**:  
  各`hourei_initial_chunk`に対して`gpt-4o`へ以下の指示：  
  「このテキスト断片に含まれる大まかな上位レベルの章・節・条などを抽出し、JSON形式で返してください。テキストが途中で終わる場合、完結した区切りまでを抽出。」  
  得られた結果を統合し、法令文書全体を包括する上位レベルセクション構造（章単位など）を確定。

- **出力**:  
  - `hourei_coarse_sections`：上位レベルセクション一覧

#### (3) 再帰的細分化 (法令側)

- **処理内容**:  
  `hourei_coarse_sections`でまだ長いセクションがあれば同様に機械的に分割し、その分割結果を`gpt-4o`にかけて次の階層レベル（例えば条→項）を抽出する。  
  この手順を繰り返し、最終的に法令文書が「条」や「項」など細粒度のセクション単位で論理的に分割される。

- **出力**:  
  - `hourei_final_sections`：論理的に細分化された法令の条項リスト

#### (4) 規定文書側も同様の段階的分割

- **処理内容**:  
  `kitei_initial_chunks`について、同様に`gpt-4o`を用いて上位レベルセクション抽出→細分化を繰り返し、規定文書を論理的に整理。
  
- **出力**:  
  - `kitei_final_sections`：論理的セクション単位の規定リスト

### 2. 法令要求事項抽出・オントロジー化

#### (5) 要求・禁止事項抽出 (`gpt-4o`)

- **処理内容**:  
  法令の最終セクションリスト(`hourei_final_sections`)を逐次`gpt-4o`に渡し、  
  「以下の条文から '〜しなければならない' (要求事項) や '〜してはならない' (禁止事項) を抽出し、JSONで返してください」  
  とプロンプト。  
  結果を集約し、全要求/禁止事項一覧を作成。

- **出力**:  
  - `requirements.json` / `prohibitions.json`

#### (6) 抽象要求具体化 (`gpt-4o`)

- **処理内容**:  
  抽象的な要求事項に対し、  
  「以下の抽象要求を満たすための具体的対策例を示してください」  
  と`gpt-4o`に問い合わせる。  
  得られた具体策をオントロジー(RDF)に反映。

- **出力**:  
  - `expanded_requirements.json`：抽象→具体要求対応表

#### (7) オントロジー統合

- **処理内容**:  
  RDFLibを用いて要求・禁止事項をRDF/OWLグラフに追加。

- **出力**: `knowledge_graph.rdf`

### 3. 規定文書セマンティック分析・Embedding化

#### (8) チャンク分割・Embedding化

- **処理内容**:  
  規定文書の`kitei_final_sections`をさらに小さめのチャンク（500~1000文字）に分割。  
  各チャンクをOpenAI Embeddings APIでEmbedding化し、ベクトルDB（Chroma等）に格納。

- **出力**: ベクトルDBインデックス (規定チャンク × Embedding)

#### (9) クラスタリング

- **処理内容**:  
  全チャンクEmbeddingを抽出、scikit-learnのKMeansでクラスタリング。  
  クラスタ内テキストを`gpt-4o`で要約し、クラスタ代表テキストと代表Embeddingを作成。

- **出力**:  
  `cluster_representatives.json`（クラスタごとの代表要約）

### 4. 要求事項マッチング・再評価

#### (10) 要求事項と規定の類似度検索

- **処理内容**:  
  各要求事項（抽象要求含む）をEmbedding化し、ベクトルDBで類似する規定チャンクまたはクラスタ代表を検索。

- **出力**:  
  要求IDごとの候補規定チャンクリスト

#### (11) LLMによる再評価 (Reranking)

- **処理内容**:  
  `gpt-4o`に「要求事項」と「候補チャンク」を提示して、  
  「この規定は要求事項を満たしますか？」  
  「法令で禁止されている行為を許容していますか？」  
  と尋ねることで最終判断。  
  必要ならスコア付け・再ランク付け。

- **出力**: `compliance_results.json`

### 5. ギャップ分析・レポート生成

#### (12) ギャップ分析 (SPARQL)

- **処理内容**:  
  RDFグラフで「未対応要求」をSPARQLクエリで抽出。

- **出力**: 未対応要求一覧

#### (13) レポート生成 (`gpt-4o`)

- **処理内容**:  
  `gpt-4o`へ分析結果（要求対応表、未対応要求、禁止事項違反有無）を渡し、  
  「この結果をもとに改善提案と最終報告書をMarkdownで作成してください」  
  と依頼。

- **出力**: `compliance_report.md`

---

## ポイントまとめ

- **gpt-4oの使用箇所**:  
  - 段階的テキスト分割：粗分割後の各チャンクごとに上位/下位セクション抽出  
  - 要求・禁止事項抽出、抽象要求具体化  
  - クラスタ代表要約  
  - 要求適合性/違反判定、レポート生成

- **トークン上限回避策**:  
  - テキストを直接一括で渡さず、物理的チャンク分割 → `gpt-4o`で部分的に処理 → 統合 → 必要なら再分割を繰り返す「階層的手法」を採用  
  - この方法で、非常に大きい文書も扱える。

- **最終成果物**:  
  - 組織内外の法令遵守状況を明確化するコンプライアンス報告書  
  - 未対応要求・問題点が明確となり、改善策も自動提案される。

